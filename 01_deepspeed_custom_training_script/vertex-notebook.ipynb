{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LLMs with DeepSpeed on Vertex AI\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using DeepSpeed on Google Cloud's Vertex AI platform.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet google-cloud-aiplatform\n",
    "!pip install --quiet datasets\n",
    "!pip install --quiet py7zr\n",
    "!pip install --quiet pandas\n",
    "!pip install --quiet python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up key variables for the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the base .env file first\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Load the .env.local file, overriding values from .env\n",
    "load_dotenv(dotenv_path=\".env.local\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"heikohotz-genai-sa\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"hf-deepspeed-training-{PROJECT_ID}\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "JOB_NAME = \"hf-deepspeed-training-job\"\n",
    "DATASET_NAME = \"timdettmers/openassistant-guanaco\"\n",
    "DATASET_FILE = f\"data/{DATASET_NAME.split('/')[-1]}/train.jsonl\"\n",
    "DATASET_PATH = f\"/gcs/{BUCKET_NAME}/{DATASET_FILE}\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "MODEL_OUTPUT_URI = f\"/gcs/{BUCKET_NAME}/{JOB_NAME}/{MODEL_NAME.split('/')[-1]}\"\n",
    "TRAINING_CONFIG_PATH = f\"{JOB_NAME}/{MODEL_NAME.split('/')[-1]}/config/training_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI SDK\n",
    "\n",
    "Connect to Vertex AI using the Google Cloud SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hugging Face Token\n",
    "\n",
    "Load the Hugging Face API token from .env file for accessing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cloud Storage Bucket\n",
    "\n",
    "Create a GCS bucket to store the training data and model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage buckets create $BUCKET_URI \\\n",
    "    --project $PROJECT_ID \\\n",
    "    --location=$REGION \\\n",
    "    --default-storage-class=STANDARD \\\n",
    "    --uniform-bucket-level-access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Download the dataset from Hugging Face and upload it to GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from google.cloud import storage\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "# Initialize a GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Save only the training split to GCS\n",
    "dataset['train'].to_json('train.jsonl', orient='records', lines=True)\n",
    "blob = bucket.blob(DATASET_FILE)\n",
    "blob.upload_from_filename('train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Configuration\n",
    "\n",
    "Upload the DeepSpeed and training configuration files to GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload training config\n",
    "blob = bucket.blob(TRAINING_CONFIG_PATH)\n",
    "blob.upload_from_filename('config/training_config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Container\n",
    "\n",
    "Build and push the custom training container to Google Container Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/training-containers/hf-training:latest\"\n",
    "!gcloud builds submit --tag {IMAGE_URI} . --project {PROJECT_ID}\n",
    "CONTAINER_URI = IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Job\n",
    "\n",
    "Set up the Vertex AI custom training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=CONTAINER_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = [\n",
    "#     # MODEL\n",
    "#     \"--model_name_or_path=meta-llama/Llama-3.1-8B\",\n",
    "#     # TRAINING\n",
    "#     \"--num_train_epochs=2\",\n",
    "#     \"--per_device_train_batch_size=2\",\n",
    "#     \"--gradient_accumulation_steps=4\",\n",
    "#     \"--gradient_checkpointing\",\n",
    "#     \"--gradient_checkpointing_use_reentrant\",\n",
    "#     \"--learning_rate=2.0e-5\",\n",
    "#     \"--lr_scheduler_type=cosine\",\n",
    "#     \"--optim=adamw_bnb_8bit\",\n",
    "#     \"--bf16\",\n",
    "#     # LOGGING\n",
    "#     \"--logging_steps=10\",\n",
    "#     \"--save_strategy=epoch\",\n",
    "#     \"--seed=42\",\n",
    "#     \"--log_level=info\",\n",
    "#     # DATASET\n",
    "#     \"--dataset_text_field=text\",\n",
    "#     \"--max_seq_length=2048\",\n",
    "#     \"--packing=false\"\n",
    "# ]\n",
    "\n",
    "args = [\n",
    "    \"--model_name_or_path\",\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"--num_train_epochs\",\n",
    "    \"2\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    \"2\",\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    \"4\",\n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--gradient_checkpointing_use_reentrant\",\n",
    "    \"--learning_rate\",\n",
    "    \"2.0e-5\",\n",
    "    \"--lr_scheduler_type\",\n",
    "    \"cosine\",\n",
    "    \"--optim\",\n",
    "    \"adamw_bnb_8bit\",\n",
    "    \"--bf16\",\n",
    "    \"--logging_steps\",\n",
    "    \"10\",\n",
    "    \"--save_strategy\",\n",
    "    \"epoch\",\n",
    "    \"--seed\",\n",
    "    \"42\",\n",
    "    \"--log_level\",\n",
    "    \"info\",\n",
    "    \"--dataset_text_field\",\n",
    "    \"text\",\n",
    "    \"--max_seq_length\",\n",
    "    \"2048\",\n",
    "    \"--packing\",\n",
    "    \"false\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Training Job\n",
    "\n",
    "Submit the training job to Vertex AI with the following specifications:\n",
    "- A2 Ultra GPU machine with 8x NVIDIA A100 80GB GPUs\n",
    "- 250GB boot disk\n",
    "- Environment variables for model training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1.types import custom_job as gca_custom_job_compat\n",
    "\n",
    "job.submit(\n",
    "    # args=args,\n",
    "    replica_count=1,\n",
    "    machine_type=\"a2-ultragpu-8g\",\n",
    "    accelerator_type=\"NVIDIA_A100_80GB\",\n",
    "    accelerator_count=8,\n",
    "    environment_variables={\n",
    "        \"HF_TOKEN\": HF_TOKEN,\n",
    "        \"TRL_USE_RICH\": \"0\",\n",
    "        \"ACCELERATE_LOG_LEVEL\": \"INFO\",\n",
    "        \"TRANSFORMERS_LOG_LEVEL\": \"INFO\",\n",
    "        \"TQDM_POSITION\": \"-1\",\n",
    "        \"DATASET_PATH\": DATASET_PATH,\n",
    "        \"MODEL_OUTPUT_DIR\": MODEL_OUTPUT_URI,\n",
    "        \"DATASET_NUMBER_OF_RECORDS\": \"2000\",\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"TRAINING_CONFIG_PATH\": f\"/gcs/{BUCKET_NAME}/{TRAINING_CONFIG_PATH}\",\n",
    "    },\n",
    "    boot_disk_size_gb=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
